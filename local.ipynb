{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys  \n",
    "# from pathlib import Path\n",
    "# sys.path[0] = str(Path(sys.path[0]).parent)\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from loguru import logger\n",
    "from collections import defaultdict\n",
    "from src.outlier_detection_methods import FewShotDetector, NaiveAggregator, local_knn\n",
    "from src.utils.utils import (\n",
    "    set_random_seed, load_model, merge_from_dict\n",
    ")\n",
    "from src.augmentations import __dict__ as AUGMENTATIONS\n",
    "from typing import Dict\n",
    "from types import SimpleNamespace\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from pathlib import Path\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from src.utils.data_fetchers import get_task_loader, get_test_features\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "dataset = \"mini_imagenet\"\n",
    "image_size = 84\n",
    "n_way = 5\n",
    "n_shot = 1\n",
    "n_query = 1\n",
    "n_tasks = 1000\n",
    "backbone = 'resnet12'\n",
    "training = 'feat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 20:34:19.030 | INFO     | __main__:<module>:3 - Loading data...\n",
      "100%|██████████| 12000/12000 [00:00<00:00, 2826509.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "\n",
    "logger.info(\"Loading data...\")\n",
    "data_loader = get_task_loader(dataset,\n",
    "                              image_size,\n",
    "                              n_way,\n",
    "                              n_shot,\n",
    "                              n_query,\n",
    "                              n_tasks,\n",
    "                              split=\"test\",\n",
    "                              n_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 20:34:19.271 | INFO     | __main__:<module>:1 - Building model...\n",
      "2022-02-03 20:34:19.272 | INFO     | src.utils.utils:load_model:110 - Fetching data...\n",
      "100%|██████████| 38400/38400 [00:00<00:00, 2792663.35it/s]\n",
      "2022-02-03 20:34:20.101 | INFO     | src.utils.utils:load_model:113 - Building model...\n",
      "2022-02-03 20:34:22.845 | INFO     | __main__:<module>:9 - Loading mean/std from base set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from data/models/resnet12_mini_imagenet_feat.pth\n",
      "Missing keys []\n",
      "Unexpected keys []\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Building model...\")\n",
    "weights = Path('data') / 'models' / f'{backbone}_{dataset}_{training}.pth'\n",
    "feature_extractor = load_model(backbone=backbone,\n",
    "                               weights=weights,\n",
    "                               dataset_name=dataset,\n",
    "                               device='cuda')\n",
    "feature_extractor.eval()\n",
    "\n",
    "logger.info(\"Loading mean/std from base set ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(images: Tensor):\n",
    "    fig = plt.figure(figsize=(15, 75))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(3, 3),  # creates 2x2 grid of axes\n",
    "                     axes_pad=0.3,  # pad between axes in inch.\n",
    "                     )\n",
    "    std = torch.Tensor([0.229, 0.224, 0.225]).to(images.device).view(3, 1, 1)\n",
    "    mean = torch.Tensor([0.485, 0.456, 0.406]).to(images.device).view(3, 1, 1)\n",
    "    shifted = images * std + mean\n",
    "    for ax, im in zip(grid, shifted):\n",
    "        # Iterating over the grid returns the Axes.\n",
    "        ax.imshow(im.permute(1, 2, 0))\n",
    "        \n",
    "def gram(t1: Tensor, t2: Tensor):\n",
    "    \"\"\"\n",
    "    t1: [N1, d]\n",
    "    t2: [N2, d]\n",
    "    returns: [N1, N2]\n",
    "    \"\"\"\n",
    "    assert len(t1.size()) == 2, \"Only accepts pooled images\"\n",
    "#     t1 = F.normalize(t1, dim=1)\n",
    "#     t2 = F.normalize(t2, dim=1)\n",
    "    return torch.cdist(t1, t2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_feature_maps(support: Tensor, query: Tensor):\n",
    "    \"\"\"\n",
    "    map_1: [Ns, c, h, w]\n",
    "    map_2: [Nq, c, h, w]\n",
    "    returns [Nq, Ns, L, h', w']\n",
    "    \"\"\"\n",
    "    support = F.normalize(support, dim=1)\n",
    "    query = F.normalize(query, dim=1)\n",
    "    N, c, h, w = support.size()\n",
    "    kh = kw = 3\n",
    "    fold_params = dict(kernel_size=(kh, kw), stride=1, dilatation=2)\n",
    "    unfold = nn.Unfold(**fold_params)\n",
    "    unfold_map1 = unfold(support) # [N, c*kw*kh, L]\n",
    "    kernels = unfold_map1.view(N, c, kh, kh, -1).permute(0, 4, 1, 2, 3) # [Ns, L, c, kw, kh]\n",
    "    conv_results = []\n",
    "    for kernel in kernels:\n",
    "        conv_results.append(F.conv2d(query, kernel)) # [Nq, L, h', w']\n",
    "    return torch.stack(conv_results, 1) # [Nq, Ns, L, h', w']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking map dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "2022-02-03 21:00:54.004 | INFO     | __main__:<module>:10 - Layer 1_0 of shape: torch.Size([5, 64, 84, 84])\n",
      "2022-02-03 21:00:54.004 | INFO     | __main__:<module>:10 - Layer 2_0 of shape: torch.Size([5, 160, 42, 42])\n",
      "2022-02-03 21:00:54.005 | INFO     | __main__:<module>:10 - Layer 3_0 of shape: torch.Size([5, 320, 21, 21])\n",
      "2022-02-03 21:00:54.005 | INFO     | __main__:<module>:10 - Layer 4_0 of shape: torch.Size([5, 640, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Just checking maps dimensions\n",
    "layers = [f'{i}_0' for i in range(1, 5)]\n",
    "with torch.no_grad():\n",
    "    for i, (support, support_labels, query, query_labels, _) in tqdm(enumerate(data_loader)):        \n",
    "        feat_s = feature_extractor(support.cuda(), layers)\n",
    "        feat_q = feature_extractor(support.cuda(), layers)\n",
    "        break\n",
    "        \n",
    "for layer in feat_s:\n",
    "    logger.info(f\"Layer {layer} of shape: {feat_s[layer].size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying this convolution idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'dilatation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-56fe236597d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfeat_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mfeat_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mconvolved_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolve_feature_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_q\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [Nq, Ns, L, h', w']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msimilarity_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolved_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mid_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_way\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_query\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-8afa8af71603>\u001b[0m in \u001b[0;36mconvolve_feature_maps\u001b[0;34m(support, query)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mkh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfold_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilatation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0munfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfold_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0munfold_map1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [N, c*kw*kh, L]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mkernels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munfold_map1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [Ns, L, c, kw, kh]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'dilatation'"
     ]
    }
   ],
   "source": [
    "layer = \"3_1\"\n",
    "id_scores = []\n",
    "ood_scores = []\n",
    "with torch.no_grad():\n",
    "    for i, (support, support_labels, query, query_labels, _) in tqdm(enumerate(data_loader)):        \n",
    "        feat_s = feature_extractor(support.cuda(), layer)[layer]\n",
    "        feat_q = feature_extractor(query.cuda(), layer)[layer]\n",
    "        convolved_res = convolve_feature_maps(feat_s, feat_q) # [Nq, Ns, L, h', w']\n",
    "        similarity_scores = convolved_res.view(feat_q.size(0), -1).max(1).values\n",
    "        id_scores.append(similarity_scores[:n_way * n_query])\n",
    "        ood_scores.append(similarity_scores[n_way * n_query:])\n",
    "        if i >= 100:\n",
    "            break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  0.,  4.,  1.,  0.,  1.,  4.,  1.,  5.,  7.,  6.,  6.,  1.,\n",
       "         6.,  8.,  7., 14.,  9., 10., 12., 11., 13.,  8., 14.,  7., 13.,\n",
       "        14., 11., 13., 16., 17., 18., 17., 10., 17.,  9., 13.,  9., 12.,\n",
       "         8.,  3.,  8.,  7.,  6.,  8.,  9.,  6.,  9.,  3., 12.,  5.,  5.,\n",
       "         4.,  7.,  5.,  4.,  5.,  3.,  1.,  3.,  4.,  1.,  4.,  2.,  3.,\n",
       "         1.,  1.,  2.,  2.,  0.,  0.,  0.,  0.,  1.,  2.,  1.,  1.,  3.,\n",
       "         0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  1.,  0.,  2.,  0.,  2.]),\n",
       " array([4.22735  , 4.2657137, 4.304077 , 4.3424406, 4.3808036, 4.419167 ,\n",
       "        4.4575305, 4.495894 , 4.5342574, 4.572621 , 4.6109843, 4.6493473,\n",
       "        4.687711 , 4.726074 , 4.7644377, 4.802801 , 4.8411646, 4.879528 ,\n",
       "        4.917891 , 4.9562545, 4.994618 , 5.0329814, 5.071345 , 5.1097083,\n",
       "        5.148072 , 5.1864347, 5.224798 , 5.2631617, 5.301525 , 5.3398886,\n",
       "        5.378252 , 5.4166155, 5.454979 , 5.493342 , 5.5317054, 5.570069 ,\n",
       "        5.6084323, 5.6467957, 5.685159 , 5.7235227, 5.7618856, 5.800249 ,\n",
       "        5.8386126, 5.876976 , 5.9153395, 5.953703 , 5.9920664, 6.0304294,\n",
       "        6.068793 , 6.1071563, 6.1455197, 6.183883 , 6.2222466, 6.26061  ,\n",
       "        6.298973 , 6.3373365, 6.3757   , 6.4140635, 6.452427 , 6.4907904,\n",
       "        6.529154 , 6.567517 , 6.6058803, 6.6442437, 6.682607 , 6.7209706,\n",
       "        6.759334 , 6.7976975, 6.8360605, 6.874424 , 6.9127874, 6.951151 ,\n",
       "        6.9895144, 7.027878 , 7.0662413, 7.1046047, 7.1429677, 7.181331 ,\n",
       "        7.2196946, 7.258058 , 7.2964215, 7.334785 , 7.3731484, 7.4115114,\n",
       "        7.449875 , 7.4882383, 7.526602 , 7.5649652, 7.6033287, 7.641692 ,\n",
       "        7.680055 , 7.7184186, 7.756782 , 7.7951455, 7.833509 , 7.8718724,\n",
       "        7.910236 , 7.948599 , 7.9869623, 8.025326 , 8.063689 ],\n",
       "       dtype=float32),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(torch.cat(id_scores).cpu().numpy(), bins=100, facecolor='g', alpha=0.5)\n",
    "plt.hist(torch.cat(ood_scores).cpu().numpy(), bins=100, facecolor='r', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
